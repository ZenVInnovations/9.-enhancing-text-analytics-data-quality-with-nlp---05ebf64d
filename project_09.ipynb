{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LOYTSwT-ymbd"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A7_eC2EYyuP-"
   },
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3lPfr1FXy08o"
   },
   "outputs": [],
   "source": [
    "# Check if spaCy model is available\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError as e:\n",
    "    logger.error(\"spaCy model 'en_core_web_sm' not found. Please install it using: python -m spacy download en_core_web_sm\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is  the best apps acording to a bunch of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a pretty good version of the game for ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is a really . there are a bunch of levels...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a silly game and can be frustrating, b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a terrific game on any pad. Hrs of fun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  label\n",
       "0  This is  the best apps acording to a bunch of ...      1\n",
       "1  This is a pretty good version of the game for ...      1\n",
       "2  this is a really . there are a bunch of levels...      1\n",
       "3  This is a silly game and can be frustrating, b...      1\n",
       "4  This is a terrific game on any pad. Hrs of fun...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('amazon.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets2 =  [{\"ticket_id\": i+7, \"description\": desc} for i, desc in enumerate(data[\"Text\"].dropna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ny2XFqoxy2Dj"
   },
   "outputs": [],
   "source": [
    "# Define sample dataset of customer support tickets\n",
    "tickets1 = [\n",
    "    {\"ticket_id\": 1, \"description\": \"Ugh, fine. You need a return label for that stupid product? Here it is #58923. Just take it\"},\n",
    "    {\"ticket_id\": 2, \"description\": \"Thank you for the quick response. I'm satisfied with the service.\"},\n",
    "    {\"ticket_id\": 3, \"description\": \"The website is not loading properly. Please fix it ASAP.\"},\n",
    "    {\"ticket_id\": 4, \"description\": \"I need to return a product. Can you send me the return label?\"},\n",
    "    {\"ticket_id\": 5, \"description\": \"Ugh, seriously? This is the best you could come up with?\"},\n",
    "    {\"ticket_id\": 6, \"description\": \"Can you help me track my order from Amazon?\"}\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets = tickets1+tickets2[:93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HSyqLTRiy5YI"
   },
   "outputs": [],
   "source": [
    "# Define NLP processing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans input text by converting to lowercase, removing punctuation and stop words,\n",
    "    and lemmatizing words using spaCy.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(text, str):\n",
    "            logger.warning(f\"Invalid text input: {text}. Returning empty string.\")\n",
    "            return \"\"\n",
    "        doc = nlp(text.lower())\n",
    "        cleaned = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "        return \" \".join(cleaned)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error cleaning text: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vr6rBqyey9tV"
   },
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    \"\"\"\n",
    "    Extracts named entities from text using spaCy's NER.\n",
    "    Returns a list of tuples (entity text, entity label).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(text, str):\n",
    "            logger.warning(f\"Invalid text input for entity extraction: {text}. Returning empty list.\")\n",
    "            return []\n",
    "        doc = nlp(text)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        return entities\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting entities: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_intent(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    if any(word in text for word in [\"return\", \"refund\", \"send back\", \"didn't like\"]):\n",
    "        return \"return_request\"\n",
    "    elif any(word in text for word in [\"thank\", \"great\", \"amazing\", \"fun\", \"enjoy\", \"love\", \"awesome\"]):\n",
    "        return \"positive_feedback\"\n",
    "    elif any(word in text for word in [\"boring\", \"bad\", \"hate\", \"crash\", \"not working\", \"waste\", \"bug\", \"issue\", \"problem\"]):\n",
    "        return \"negative_feedback\"\n",
    "    elif any(word in text for word in [\"help\", \"how\", \"can you\", \"support\", \"assist\", \"instructions\"]):\n",
    "        return \"inquiry\"\n",
    "    elif any(word in text for word in [\"delay\", \"late\", \"not arrived\", \"shipping\", \"delivery\"]):\n",
    "        return \"order_issue\"\n",
    "    elif any(word in text for word in [\"too many ads\", \"ads are annoying\", \"ads interrupt\", \"ads ruin\", \"ads are bad\"]):\n",
    "        return \"ads_complaint\"\n",
    "    else:\n",
    "        return \"general_comment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since SentimnetIntensityAnalyzer from vanderSentiment provided better results, it has been used instead of textBlob for sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TJ4kWz1My_UV"
   },
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Computes sentiment polarity of text using TextBlob.\n",
    "    Returns a float between -1 (negative) and 1 (positive).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(text, str):\n",
    "            logger.warning(f\"Invalid text input for sentiment analysis: {text}. Returning 0.0.\")\n",
    "            return 0.0\n",
    "        # blob = TextBlob(text)\n",
    "        # return blob.sentiment.polarity\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        score = analyzer.polarity_scores(text)\n",
    "        del score['compound']\n",
    "        max_key = max(score,key=score.get)\n",
    "        return score[max_key]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error computing sentiment: {str(e)}\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ed76Bsa8zCmF"
   },
   "outputs": [],
   "source": [
    "def get_sentiment_label(text):\n",
    "    \"\"\"\n",
    "    Converts sentiment score to a label (Positive, Negative, Neutral).\n",
    "    \"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    del score['compound']\n",
    "    return max(score,key=score.get)\n",
    "    try:\n",
    "        if not isinstance(text, str):\n",
    "            logger.warning(f\"Invalid text input for sentiment analysis: {text}. Returning neu.\")\n",
    "            return neu\n",
    "        # blob = TextBlob(text)\n",
    "        # return blob.sentiment.polarity\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        score = analyzer.polarity_scores(text)\n",
    "        del score['compound']\n",
    "        return max(score,key=score.get)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error labeling sentiment: {str(e)}\")\n",
    "        return \"Neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "N6rsSXO0zIcc"
   },
   "outputs": [],
   "source": [
    "def extract_order_number(text):\n",
    "    \"\"\"\n",
    "    Extracts order numbers from text using a regular expression.\n",
    "    Assumes order numbers are in the format # followed by digits.\n",
    "    Returns the order number if found, otherwise None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(text, str):\n",
    "            logger.warning(f\"Invalid text input for order number extraction: {text}. Returning None.\")\n",
    "            return None\n",
    "        pattern = r\"#(\\d+)\"\n",
    "        match = re.search(pattern, text)\n",
    "        return match.group(1) if match else None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting order number: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QdKA0GzzLFw",
    "outputId": "683883e6-d0fd-4819-e592-1ff272963051"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 12:41:33,105 - INFO - Creating DataFrame from sample dataset\n",
      "2025-05-17 12:41:33,105 - INFO - Applying NLP processing to dataset\n",
      "2025-05-17 12:41:40,839 - INFO - Displaying enriched dataset\n",
      "2025-05-17 12:41:40,858 - INFO - Saving enriched dataset to enriched_tickets.csv\n",
      "2025-05-17 12:41:40,873 - INFO - Dataset successfully saved to enriched_tickets.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enriched Dataset:\n",
      "    ticket_id                                        description  \\\n",
      "0           1  Ugh, fine. You need a return label for that st...   \n",
      "1           2  Thank you for the quick response. I'm satisfie...   \n",
      "2           3  The website is not loading properly. Please fi...   \n",
      "3           4  I need to return a product. Can you send me th...   \n",
      "4           5  Ugh, seriously? This is the best you could com...   \n",
      "..        ...                                                ...   \n",
      "94         95  Hey it's Angry Birds, what's not to love!  Eas...   \n",
      "95         96  I can't believe how  hard I try to hit the tar...   \n",
      "96         97  I didn't like it at all .I think it is more fo...   \n",
      "97         98  I got to the end of he pre-installed version. ...   \n",
      "98         99  I hate it they say no at everything l got rid ...   \n",
      "\n",
      "                                  cleaned_description  \\\n",
      "0     ugh fine need return label stupid product 58923   \n",
      "1              thank quick response satisfied service   \n",
      "2                      website load properly fix asap   \n",
      "3               need return product send return label   \n",
      "4                             ugh seriously good come   \n",
      "..                                                ...   \n",
      "94  hey angry bird love   easy use new kindle hd s...   \n",
      "95  believe   hard try hit target good laugh onese...   \n",
      "96                             like .i think kid game   \n",
      "97  get end pre installed version confuse check on...   \n",
      "98  hate l get rid shut kindle stupid $ $ $ $ $ 97...   \n",
      "\n",
      "                                             entities  sentiment_score  \\\n",
      "0                                    [(58923, MONEY)]            0.652   \n",
      "1                                                  []            0.629   \n",
      "2                                                  []            0.821   \n",
      "3                                                  []            1.000   \n",
      "4                                                  []            0.469   \n",
      "..                                                ...              ...   \n",
      "94                [(Angry Birds, ORG), (hours, TIME)]            0.698   \n",
      "95                                                 []            0.700   \n",
      "96                                                 []            0.900   \n",
      "97                                  [(One, CARDINAL)]            0.883   \n",
      "98  [(everything l got rid, ORG), ($$$$$*, MONEY),...            0.568   \n",
      "\n",
      "   sentiment_label order_number             intent  \n",
      "0              neg        58923     return_request  \n",
      "1              pos         None  positive_feedback  \n",
      "2              neu         None    general_comment  \n",
      "3              neu         None     return_request  \n",
      "4              neg         None    general_comment  \n",
      "..             ...          ...                ...  \n",
      "94             neu         None  positive_feedback  \n",
      "95             pos         None            inquiry  \n",
      "96             neu         None     return_request  \n",
      "97             neu         None  negative_feedback  \n",
      "98             neu         9733  negative_feedback  \n",
      "\n",
      "[99 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process the dataset and enhance text analytics data quality.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a pandas DataFrame from the sample dataset\n",
    "        logger.info(\"Creating DataFrame from sample dataset\")\n",
    "        df = pd.DataFrame(tickets)\n",
    "\n",
    "        # Apply NLP functions to enhance the dataset\n",
    "        logger.info(\"Applying NLP processing to dataset\")\n",
    "        df['cleaned_description'] = df['description'].apply(clean_text)\n",
    "        df['entities'] = df['description'].apply(extract_entities)\n",
    "        df['sentiment_score'] = df['description'].apply(get_sentiment)\n",
    "        df['sentiment_label'] = df['cleaned_description'].apply(get_sentiment_label)\n",
    "        df['order_number'] = df['description'].apply(extract_order_number)\n",
    "        df['intent'] = df['description'].apply(identify_intent)\n",
    "\n",
    "        # Display the enriched dataset\n",
    "        logger.info(\"Displaying enriched dataset\")\n",
    "        print(\"\\nEnriched Dataset:\")\n",
    "        print(df)\n",
    "\n",
    "        # Save the enriched dataset to a CSV file\n",
    "        output_path = Path(\"enriched_tickets.csv\")\n",
    "        logger.info(f\"Saving enriched dataset to {output_path}\")\n",
    "        df.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Dataset successfully saved to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main processing: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
